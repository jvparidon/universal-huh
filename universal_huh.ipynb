{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding cross-linguistic analogues to \"huh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "1. Implement whitespace-sensitive method for identifying dialogue ABA (as opposed to monologue ABA)\n",
    "2. Check if that changes the rankings?\n",
    "3. Check if Google Translate has an audio API\n",
    "4. Maybe add 1 non-alphabetic language to the initial (pre-preregistration) dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "def display_md(md, **kwargs):\n",
    "    return display_markdown(md, raw=True, **kwargs)\n",
    "\n",
    "langs = ['af', 'fi', 'nl', 'tr']  # these are the languages we will be examining in this pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repair initiation in dialogue method\n",
    "To find repair initiation in our subtitle corpus, we use the following sequence of steps:\n",
    "\n",
    "1. Parse subtitle files and extract the text and timestamp for each utterance\n",
    "2. Find all ABA sequences, meaning an utterance A, followed by an utterance B, followed by the identical utterance A\n",
    "3. Reject B if it is identical to A (making it an AAA sequence)\n",
    "4. Reject B if it is more than one word\n",
    "5. Reject B if it is not a question (does not end in \"?\")\n",
    "6. Count frequencies of B\n",
    "\n",
    "These Bs are potential repair initiations; they are one-word questions that provoke a repetition of the preceding utterance. A possible weakness in this method is that straight repeats aren't necessarily very good repairs, but it is very difficult to define a usable rule to search for sequences that would constitute good repairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ABA sequence B-word frequencies for language: af"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "No ABA sequences found for language: af"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ABA sequence B-word frequencies for language: fi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word      |   count |\n",
       "|------:|:----------|--------:|\n",
       "|     0 | mitä?     |    1410 |\n",
       "|     1 | mikä?     |     120 |\n",
       "|     2 | anteeksi? |     119 |\n",
       "|     3 | haloo?    |      81 |\n",
       "|     4 | kuka?     |      59 |\n",
       "|     5 | miksi?    |      57 |\n",
       "|     6 | niinkö?   |      55 |\n",
       "|     7 | niin?     |      49 |\n",
       "|     8 | missä?    |      48 |\n",
       "|     9 | kuuletko? |      48 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ABA sequence B-word frequencies for language: nl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word    |   count |\n",
       "|------:|:--------|--------:|\n",
       "|     0 | wat?    |    2043 |\n",
       "|     1 | pardon? |     254 |\n",
       "|     2 | ja?     |     148 |\n",
       "|     3 | wie?    |     141 |\n",
       "|     4 | echt?   |     128 |\n",
       "|     5 | hallo?  |     124 |\n",
       "|     6 | nee?    |     112 |\n",
       "|     7 | ?       |      88 |\n",
       "|     8 | waarom? |      80 |\n",
       "|     9 | oké?    |      78 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ABA sequence B-word frequencies for language: tr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word       |   count |\n",
       "|------:|:-----------|--------:|\n",
       "|     0 | ne?        |    4186 |\n",
       "|     1 | efendim?   |     310 |\n",
       "|     2 | neden?     |     248 |\n",
       "|     3 | kim?       |     212 |\n",
       "|     4 | ?          |     205 |\n",
       "|     5 | nerede?    |     203 |\n",
       "|     6 | evet?      |     183 |\n",
       "|     7 | alo?       |     167 |\n",
       "|     8 | ha?        |     166 |\n",
       "|     9 | neredesin? |     160 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    display_md(f'### ABA sequence B-word frequencies for language: {lang}')\n",
    "    df = pd.read_csv(f'dialogue_method/{lang}_freqs.tsv', sep='\\t').rename_axis('idx')\n",
    "    if len(df) > 0:\n",
    "        display_md(df.head(10).to_markdown())\n",
    "    else:\n",
    "        display_md(f'No ABA sequences found for language: {lang}')\n",
    "    display_md('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afrikaans seems to have too small a corpus to yield any ABA-sequences, but in all three of the other languages tested here, the highest frequency B-word is the equivalent of \"what\" (i.e., \"wat\", \"mitä\", and \"ne\" all mean \"what\").  \n",
    "Asking \"what?\" is not an unreasonable way to initiate a repair, but of course we were looking for \"huh\" equivalents.  \n",
    "We can try our procedure on English, to see what kind of results we get for the language the majority of subtitles are translated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ABA sequence B-word frequencies for language: en"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word    |   count |\n",
       "|------:|:--------|--------:|\n",
       "|     0 | what?   |   10455 |\n",
       "|     1 | ?       |    4109 |\n",
       "|     2 | huh?    |    1988 |\n",
       "|     3 | yeah?   |    1181 |\n",
       "|     4 | really? |    1116 |\n",
       "|     5 | why?    |    1114 |\n",
       "|     6 | hello?  |     980 |\n",
       "|     7 | no?     |     779 |\n",
       "|     8 | okay?   |     737 |\n",
       "|     9 | who?    |     698 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang = 'en'\n",
    "display_md(f'### ABA sequence B-word frequencies for language: {lang}')\n",
    "display_md(pd.read_csv(f'dialogue_method/{lang}_freqs.tsv', sep='\\t').rename_axis('idx').head(10).to_markdown())\n",
    "display_md('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in English \"what\" is the most frequent B-word, by far.  \n",
    "However, for English \"huh\" is the third item; relatively much more frequent than in other languages. Two factors might partially explain why \"what\" is more prevalent than \"huh\" in English and even more so in other languages.\n",
    "\n",
    "1. Scriptwriters prefer to use a more lexicalized repair initiation, whereas in real life people are likely to use \"huh?\" and its equivalents.\n",
    "2. That same cognitive bias toward lexicalized repair initiations affects translators. As dialogue is transcribed and translated, \"huh\" sometimes gets turned into \"what\", for instance. This results in a further decrease in the fraction of \"huh\" equivalents.\n",
    "\n",
    "(It's worth noting that the second entry, \"NaN\" is actually just a solitary question mark (\"?\") instead of an actual word. It's unclear what these question marks represent. It could be any kind of communicative act, possibly including utterances like \"huh\".)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-linguistic nearest neighbor method\n",
    "This method makes use of word embeddings, vector representations of words, to find cross-linguistic equivalents of \"huh\".\n",
    "\n",
    "1. Train word embeddings on subtitles (and Wikipedia, to increase the size of the training corpus) in every available language\n",
    "2. Align embeddings in different languages to the English embeddings (by iteratively shrinking the cosine distance between dictionary-derived word translation pairs)\n",
    "3. Find cross-linguistic nearest neighbor of \"huh\" in every English/other language pair using cosine distances\n",
    "\n",
    "Word embedding vectors are a numerical representation of the typical context a word is found in. Given that equivalents of \"huh\" are likely to occur in similar contexts to \"huh\", we expect the word vectors (in aligned spaces) to be highly similar, reflected in a small cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Cross-linguistic cosine distances for language: af"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word        |   cosine_distance |\n",
       "|------:|:------------|------------------:|\n",
       "|     0 | Hê          |          0.2196   |\n",
       "|     1 | liefie      |          0.237689 |\n",
       "|     2 | nê          |          0.238153 |\n",
       "|     3 | Pappatjie   |          0.240027 |\n",
       "|     4 | slivovitz   |          0.244561 |\n",
       "|     5 | my_liefling |          0.247401 |\n",
       "|     6 | Mammie      |          0.24974  |\n",
       "|     7 | nè          |          0.252405 |\n",
       "|     8 | jou         |          0.255947 |\n",
       "|     9 | my          |          0.256397 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross-linguistic cosine distances for language: fi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word        |   cosine_distance |\n",
       "|------:|:------------|------------------:|\n",
       "|     0 | poju        |          0.214634 |\n",
       "|     1 | kamu        |          0.223444 |\n",
       "|     2 | häh         |          0.225935 |\n",
       "|     3 | ystäväiseni |          0.227547 |\n",
       "|     4 | kultsi      |          0.233309 |\n",
       "|     5 | pikkukaveri |          0.23373  |\n",
       "|     6 | helkkari    |          0.234174 |\n",
       "|     7 | pikkumies   |          0.235422 |\n",
       "|     8 | helkkarin   |          0.235567 |\n",
       "|     9 | herraseni   |          0.239839 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross-linguistic cosine distances for language: nl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word      |   cosine_distance |\n",
       "|------:|:----------|------------------:|\n",
       "|     0 | huh       |          0.159184 |\n",
       "|     1 | schatje   |          0.162617 |\n",
       "|     2 | lieverd   |          0.174831 |\n",
       "|     3 | liefje    |          0.178294 |\n",
       "|     4 | hé        |          0.180575 |\n",
       "|     5 | hè        |          0.181786 |\n",
       "|     6 | makker    |          0.187827 |\n",
       "|     7 | meneertje |          0.194914 |\n",
       "|     8 | mam       |          0.202271 |\n",
       "|     9 | oké       |          0.202835 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Cross-linguistic cosine distances for language: tr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word       |   cosine_distance |\n",
       "|------:|:-----------|------------------:|\n",
       "|     0 | dostum     |          0.217794 |\n",
       "|     1 | </s>       |          0.226429 |\n",
       "|     2 | ha         |          0.231013 |\n",
       "|     3 | ahbap      |          0.23277  |\n",
       "|     4 | huh        |          0.237431 |\n",
       "|     5 | adamım     |          0.239443 |\n",
       "|     6 | koca_oğlan |          0.242911 |\n",
       "|     7 | tatlım     |          0.247647 |\n",
       "|     8 | Hey        |          0.250283 |\n",
       "|     9 | beyler     |          0.253327 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    display_md(f'### Cross-linguistic cosine distances for language: {lang}')\n",
    "    display_md(pd.read_csv(f'embedding_method/en-{lang}_muse_neighbors.tsv', sep='\\t').rename_axis('idx').head(10).to_markdown())\n",
    "    display_md('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are a lot closer than the dialogue method results. For Afrikaans and Dutch we get the correct answer in first place, for Turkish and Finnish the correct answer is in third place.  \n",
    "Because this method is based on distances between language pairs, there is no set of English results (the cosine distance would be zero).\n",
    "\n",
    "Note that many of the lower-ranked but still close neighbors appear to be in a distinct semantic category: words equivalent to \"honey\", \"buddy\", \"mommy\" and the like. Maybe we can call this category 'terms of endearment'?  \n",
    "It's worth pondering why these words, in particular, seem close to huh. My hypothesis is that they occur in a similar context to \"huh\" in that they are used in a wide variety of contexts, but often either in isolation or at the end of sentences, just like \"huh\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining evidence from both methods\n",
    "Neither of the methods used above is perfect, but since they provide evidence from different angles, we can combine the rankings they provide to come to a better answer. We simply sum the rankings of the top 100 answers generated by each method and look for the lowest summed rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Combined rankings for language: af"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "No combined rankings found for language: af"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Combined rankings for language: fi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word   |   count |   cosine_distance |   d_rank |   e_rank |   sum_rank |\n",
       "|------:|:-------|--------:|------------------:|---------:|---------:|-----------:|\n",
       "|     0 | häh    |      13 |          0.225935 |       25 |        2 |         27 |\n",
       "|     1 | nyt    |       7 |          0.247161 |       58 |       18 |         76 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Combined rankings for language: nl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word   |   count |   cosine_distance |   d_rank |   e_rank |   sum_rank |\n",
       "|------:|:-------|--------:|------------------:|---------:|---------:|-----------:|\n",
       "|     0 | huh    |      33 |          0.159184 |       17 |        0 |         17 |\n",
       "|     1 | oké    |      78 |          0.202835 |        9 |        9 |         18 |\n",
       "|     2 | mam    |      28 |          0.202271 |       20 |        8 |         28 |\n",
       "|     3 | pap    |      23 |          0.211262 |       27 |       13 |         40 |\n",
       "|     4 | hè     |      13 |          0.181786 |       40 |        5 |         45 |\n",
       "|     5 | papa   |      14 |          0.218817 |       39 |       19 |         58 |\n",
       "|     6 | hé     |       8 |          0.180575 |       75 |        4 |         79 |\n",
       "|     7 | eh     |       7 |          0.213423 |       83 |       15 |         98 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Combined rankings for language: tr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   idx | word   |   count |   cosine_distance |   d_rank |   e_rank |   sum_rank |\n",
       "|------:|:-------|--------:|------------------:|---------:|---------:|-----------:|\n",
       "|     0 | ha     |     166 |          0.231013 |        8 |        2 |         10 |\n",
       "|     1 | huh    |      26 |          0.237431 |       28 |        4 |         32 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    display_md(f'### Combined rankings for language: {lang}')\n",
    "    df = pd.read_csv(f'{lang}_ranks.tsv', sep='\\t').rename_axis('idx')\n",
    "    if len(df) > 0:\n",
    "        display_md(df.head(10).to_markdown())\n",
    "    else:\n",
    "        display_md(f'No combined rankings found for language: {lang}')\n",
    "    display_md('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this method yields the correct answer in each language. (We can disregard Afrikaans, because there  were no hits in Afrikaans using the dialogue method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Jupyter notebook converted to Markdown successfully."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert this Jupyter notebook to Markdown\n",
    "import subprocess as sp\n",
    "make_md = 'jupyter nbconvert universal_huh.ipynb --to markdown --output universal_huh.md'.split(' ')\n",
    "convert = sp.run(make_md)\n",
    "if convert.returncode == 0:\n",
    "    display_md('Jupyter notebook converted to Markdown successfully.')\n",
    "else:\n",
    "    display_md('Error: encountered problem converting Jupyter notebook to Markdown')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
